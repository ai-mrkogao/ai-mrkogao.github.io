---
title: "Asynchronous Agent Actor Critic (A3C)"
date: 2018-12-21
classes: wide
use_math: true
tags: reinforcement_learning actor_critic policy_gradient DDPG
category: reinforcement learning
---


[Asynchronous Agent Actor Critic (A3C)](https://hdmetor.github.io/a3c-explained/)

# Asynchronous Agent Actor Critic (A3C)

## Reinforcement Learning refresh

Let's briefly review what reinforcement is, and what problems it tries to solve. 

The general idea, is to have an agent that can interact in an environment by performing some action $a$. The effect of performing such action is to receive a reward $r$ and a new state $s'$, so that the cycle continues.

More formally we have:

- a discrete time indexed by $t$
- an environment $E$, made by a set of states $\{s_t\}$
- a set of actions  $\{a_t\}$
- policy $\pi: s_t \longrightarrow a_t$, which describes what action should be taken in each state
- a reward $r_t$ given after each action is performed

At each time stamp $t$, the agent will try to maximize the _future discounted reward_ defined as

$$R_t = \sum_{k =0 }^{\infty} \gamma^k r_{k+t}$$

where $\gamma \in [0,1]$ is the discount factor.

Please note that the previous sum is in reality a _finite_ sum because we can assume that the episode will terminate at some point. In the case of Atari games, each game will terminate either with a win or loss.
