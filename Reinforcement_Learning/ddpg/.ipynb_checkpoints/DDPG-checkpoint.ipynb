{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "usage: ipykernel_launcher.py [-h] [--actor-lr ACTOR_LR]\n",
      "                             [--critic-lr CRITIC_LR] [--gamma GAMMA]\n",
      "                             [--tau TAU] [--buffer-size BUFFER_SIZE]\n",
      "                             [--minibatch-size MINIBATCH_SIZE] [--env ENV]\n",
      "                             [--random-seed RANDOM_SEED]\n",
      "                             [--max-episodes MAX_EPISODES]\n",
      "                             [--max-episode-len MAX_EPISODE_LEN]\n",
      "                             [--render-env] [--use-gym-monitor]\n",
      "                             [--monitor-dir MONITOR_DIR]\n",
      "                             [--summary-dir SUMMARY_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-9d770238-f696-4f62-89a3-62b17d6f5b3c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Implementation of DDPG - Deep Deterministic Policy Gradient\n",
    "\n",
    "Algorithm and hyperparameter details can be found here: \n",
    "    http://arxiv.org/pdf/1509.02971v2.pdf\n",
    "\n",
    "The algorithm is tested on the Pendulum-v0 OpenAI gym task \n",
    "and developed with tflearn + Tensorflow\n",
    "\n",
    "Author: Patrick Emami\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "# ===========================\n",
    "#   Actor and Critic DNNs\n",
    "# ===========================\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "            len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.fully_connected(net, 300)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(\n",
    "            net, self.a_dim, activation='tanh', weights_init=w_init)\n",
    "        # Scale output to -action_bound to action_bound\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "\n",
    "        # Add the action tensor in the 2nd hidden layer\n",
    "        # Use two temp layers to get the corresponding weights and biases\n",
    "        t1 = tflearn.fully_connected(net, 300)\n",
    "        t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "        net = tflearn.activation(\n",
    "            tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
    "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "# ===========================\n",
    "#   Tensorflow Summary Ops\n",
    "# ===========================\n",
    "\n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "# ===========================\n",
    "#   Agent Training\n",
    "# ===========================\n",
    "\n",
    "def train(sess, env, args, actor, critic, actor_noise):\n",
    "\n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(args['summary_dir'], sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(int(args['buffer_size']), int(args['random_seed']))\n",
    "\n",
    "    # Needed to enable BatchNorm. \n",
    "    # This hurts the performance on Pendulum but could be useful\n",
    "    # in other environments.\n",
    "    # tflearn.is_training(True)\n",
    "\n",
    "    for i in range(int(args['max_episodes'])):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in range(int(args['max_episode_len'])):\n",
    "\n",
    "            if args['render_env']:\n",
    "                env.render()\n",
    "\n",
    "            # Added exploration noise\n",
    "            #a = actor.predict(np.reshape(s, (1, 3))) + (1. / (1. + i))\n",
    "            a = actor.predict(np.reshape(s, (1, actor.s_dim))) + actor_noise()\n",
    "\n",
    "            s2, r, terminal, info = env.step(a[0])\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > int(args['minibatch_size']):\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                    replay_buffer.sample_batch(int(args['minibatch_size']))\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = critic.predict_target(\n",
    "                    s2_batch, actor.predict_target(s2_batch))\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(int(args['minibatch_size'])):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = critic.train(\n",
    "                    s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)))\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                # Update the actor policy using the sampled gradient\n",
    "                a_outs = actor.predict(s_batch)\n",
    "                grads = critic.action_gradients(s_batch, a_outs)\n",
    "                actor.train(s_batch, grads[0])\n",
    "\n",
    "                # Update target networks\n",
    "                actor.update_target_network()\n",
    "                critic.update_target_network()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal:\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j)\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, i)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f}'.format(int(ep_reward), \\\n",
    "                        i, (ep_ave_max_q / float(j))))\n",
    "                break\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        env = gym.make(args['env'])\n",
    "        np.random.seed(int(args['random_seed']))\n",
    "        tf.set_random_seed(int(args['random_seed']))\n",
    "        env.seed(int(args['random_seed']))\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        action_bound = env.action_space.high\n",
    "        # Ensure action bound is symmetric\n",
    "        assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "        actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                             float(args['actor_lr']), float(args['tau']),\n",
    "                             int(args['minibatch_size']))\n",
    "\n",
    "        critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                               float(args['critic_lr']), float(args['tau']),\n",
    "                               float(args['gamma']),\n",
    "                               actor.get_num_trainable_vars())\n",
    "        \n",
    "        actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "        if args['use_gym_monitor']:\n",
    "            if not args['render_env']:\n",
    "                env = wrappers.Monitor(\n",
    "                    env, args['monitor_dir'], video_callable=False, force=True)\n",
    "            else:\n",
    "                env = wrappers.Monitor(env, args['monitor_dir'], force=True)\n",
    "\n",
    "        train(sess, env, args, actor, critic, actor_noise)\n",
    "\n",
    "        if args['use_gym_monitor']:\n",
    "            env.monitor.close()\n",
    "def start():\n",
    "# if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='provide arguments for DDPG agent')\n",
    "\n",
    "    # agent parameters\n",
    "    parser.add_argument('--actor-lr', help='actor network learning rate', default=0.0001)\n",
    "    parser.add_argument('--critic-lr', help='critic network learning rate', default=0.001)\n",
    "    parser.add_argument('--gamma', help='discount factor for critic updates', default=0.99)\n",
    "    parser.add_argument('--tau', help='soft target update parameter', default=0.001)\n",
    "    parser.add_argument('--buffer-size', help='max size of the replay buffer', default=1000000)\n",
    "    parser.add_argument('--minibatch-size', help='size of minibatch for minibatch-SGD', default=64)\n",
    "\n",
    "    # run parameters\n",
    "    parser.add_argument('--env', help='choose the gym env- tested on {Pendulum-v0}', default='Pendulum-v0')\n",
    "    parser.add_argument('--random-seed', help='random seed for repeatability', default=1234)\n",
    "    parser.add_argument('--max-episodes', help='max num of episodes to do while training', default=50000)\n",
    "    parser.add_argument('--max-episode-len', help='max length of 1 episode', default=1000)\n",
    "    parser.add_argument('--render-env', help='render the gym env', action='store_true')\n",
    "    parser.add_argument('--use-gym-monitor', help='record gym results', action='store_true')\n",
    "    parser.add_argument('--monitor-dir', help='directory for storing gym results', default='./results/gym_ddpg')\n",
    "    parser.add_argument('--summary-dir', help='directory for storing tensorboard info', default='./results/tf_ddpg')\n",
    "\n",
    "    parser.set_defaults(render_env=True)\n",
    "    parser.set_defaults(use_gym_monitor=True)\n",
    "    \n",
    "    args = vars(parser.parse_args())\n",
    "    \n",
    "    pp.pprint(args)\n",
    "\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actor_lr': 0.0001,\n",
      " 'buffer_size': 1000000,\n",
      " 'critic_lr': 0.001,\n",
      " 'env': 'Pendulum-v0',\n",
      " 'gamma': 0.99,\n",
      " 'max_episode_len': 1000,\n",
      " 'max_episodes': '1000',\n",
      " 'minibatch_size': 64,\n",
      " 'monitor_dir': './results/gym_ddpg',\n",
      " 'random_seed': 1234,\n",
      " 'render_env': True,\n",
      " 'summary_dir': './results/tf_ddpg',\n",
      " 'tau': 0.001,\n",
      " 'use_gym_monitor': True}\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Summary name Qmax Value is illegal; using Qmax_Value instead.\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "| Reward: -1102 | Episode: 0 | Qmax: -0.2419\n",
      "| Reward: -1796 | Episode: 1 | Qmax: -0.5069\n",
      "| Reward: -1179 | Episode: 2 | Qmax: -0.5815\n",
      "| Reward: -1901 | Episode: 3 | Qmax: -0.5588\n",
      "| Reward: -1542 | Episode: 4 | Qmax: -0.5314\n",
      "| Reward: -1447 | Episode: 5 | Qmax: -0.5938\n",
      "| Reward: -1561 | Episode: 6 | Qmax: -0.6973\n",
      "| Reward: -1601 | Episode: 7 | Qmax: -0.9144\n",
      "| Reward: -1597 | Episode: 8 | Qmax: -1.1198\n",
      "| Reward: -1516 | Episode: 9 | Qmax: -1.1114\n",
      "| Reward: -1466 | Episode: 10 | Qmax: -1.3585\n",
      "| Reward: -1471 | Episode: 11 | Qmax: -1.5295\n",
      "| Reward: -1510 | Episode: 12 | Qmax: -1.5520\n",
      "| Reward: -1519 | Episode: 13 | Qmax: -1.8575\n",
      "| Reward: -1489 | Episode: 14 | Qmax: -2.0767\n",
      "| Reward: -1619 | Episode: 15 | Qmax: -2.3029\n",
      "| Reward: -1286 | Episode: 16 | Qmax: -3.0033\n",
      "| Reward: -697 | Episode: 17 | Qmax: -2.5230\n",
      "| Reward: -1071 | Episode: 18 | Qmax: -1.8527\n",
      "| Reward: -1182 | Episode: 19 | Qmax: -2.0651\n",
      "| Reward: -1182 | Episode: 20 | Qmax: -2.0979\n",
      "| Reward: -1045 | Episode: 21 | Qmax: -1.9790\n",
      "| Reward: -1089 | Episode: 22 | Qmax: -2.4298\n",
      "| Reward: -1295 | Episode: 23 | Qmax: -2.4820\n",
      "| Reward: -1401 | Episode: 24 | Qmax: -2.5877\n",
      "| Reward: -1379 | Episode: 25 | Qmax: -2.5015\n",
      "| Reward: -870 | Episode: 26 | Qmax: -2.3820\n",
      "| Reward: -1540 | Episode: 27 | Qmax: -2.4663\n",
      "| Reward: -1132 | Episode: 28 | Qmax: -2.3637\n",
      "| Reward: -1163 | Episode: 29 | Qmax: -2.7532\n",
      "| Reward: -1068 | Episode: 30 | Qmax: -2.6124\n",
      "| Reward: -1494 | Episode: 31 | Qmax: -3.0462\n",
      "| Reward: -941 | Episode: 32 | Qmax: -3.1068\n",
      "| Reward: -893 | Episode: 33 | Qmax: -3.2663\n",
      "| Reward: -981 | Episode: 34 | Qmax: -3.5665\n",
      "| Reward: -972 | Episode: 35 | Qmax: -3.5290\n",
      "| Reward: -749 | Episode: 36 | Qmax: -2.5504\n",
      "| Reward: -961 | Episode: 37 | Qmax: -3.0555\n",
      "| Reward: -992 | Episode: 38 | Qmax: -2.7338\n",
      "| Reward: -1507 | Episode: 39 | Qmax: -3.2037\n",
      "| Reward: -776 | Episode: 40 | Qmax: -3.0621\n",
      "| Reward: -939 | Episode: 41 | Qmax: -3.3512\n",
      "| Reward: -767 | Episode: 42 | Qmax: -3.4451\n",
      "| Reward: -903 | Episode: 43 | Qmax: -3.2013\n",
      "| Reward: -722 | Episode: 44 | Qmax: -3.0913\n",
      "| Reward: -757 | Episode: 45 | Qmax: -3.2600\n",
      "| Reward: -778 | Episode: 46 | Qmax: -2.8136\n",
      "| Reward: -641 | Episode: 47 | Qmax: -3.4971\n",
      "| Reward: -1506 | Episode: 48 | Qmax: -2.4936\n",
      "| Reward: -551 | Episode: 49 | Qmax: -2.4314\n",
      "| Reward: -1577 | Episode: 50 | Qmax: -1.9211\n",
      "| Reward: -831 | Episode: 51 | Qmax: -2.4247\n",
      "| Reward: -1520 | Episode: 52 | Qmax: -1.8314\n",
      "| Reward: -519 | Episode: 53 | Qmax: -1.8659\n",
      "| Reward: -514 | Episode: 54 | Qmax: -1.2503\n",
      "| Reward: -655 | Episode: 55 | Qmax: -0.7697\n",
      "| Reward: -770 | Episode: 56 | Qmax: -0.5899\n",
      "| Reward: -398 | Episode: 57 | Qmax: -0.6120\n",
      "| Reward: -507 | Episode: 58 | Qmax: 0.0862\n",
      "| Reward: -529 | Episode: 59 | Qmax: 0.1987\n",
      "| Reward: -351 | Episode: 60 | Qmax: 0.3151\n",
      "| Reward: -337 | Episode: 61 | Qmax: 0.5641\n",
      "| Reward: -501 | Episode: 62 | Qmax: 0.6050\n",
      "| Reward: -389 | Episode: 63 | Qmax: 0.8992\n",
      "| Reward: -262 | Episode: 64 | Qmax: 1.1488\n",
      "| Reward: -263 | Episode: 65 | Qmax: 1.1349\n",
      "| Reward: -259 | Episode: 66 | Qmax: 1.2574\n",
      "| Reward: -392 | Episode: 67 | Qmax: 1.2814\n",
      "| Reward: -131 | Episode: 68 | Qmax: 1.4261\n",
      "| Reward: -135 | Episode: 69 | Qmax: 1.5094\n",
      "| Reward: -654 | Episode: 70 | Qmax: 1.6579\n",
      "| Reward: -265 | Episode: 71 | Qmax: 1.6770\n",
      "| Reward: -388 | Episode: 72 | Qmax: 1.7222\n",
      "| Reward: -631 | Episode: 73 | Qmax: 1.7442\n",
      "| Reward: -136 | Episode: 74 | Qmax: 1.8081\n",
      "| Reward: -650 | Episode: 75 | Qmax: 1.8158\n",
      "| Reward: -259 | Episode: 76 | Qmax: 1.8422\n",
      "| Reward: -5 | Episode: 77 | Qmax: 1.8536\n",
      "| Reward: -125 | Episode: 78 | Qmax: 1.9610\n",
      "| Reward: -2 | Episode: 79 | Qmax: 1.9939\n",
      "| Reward: -501 | Episode: 80 | Qmax: 2.0305\n",
      "| Reward: -3 | Episode: 81 | Qmax: 2.0379\n",
      "| Reward: -129 | Episode: 82 | Qmax: 2.0830\n",
      "| Reward: -319 | Episode: 83 | Qmax: 2.1072\n",
      "| Reward: -424 | Episode: 84 | Qmax: 2.1594\n",
      "| Reward: -254 | Episode: 85 | Qmax: 2.1648\n",
      "| Reward: -245 | Episode: 86 | Qmax: 2.2042\n",
      "| Reward: -134 | Episode: 87 | Qmax: 2.1768\n",
      "| Reward: -281 | Episode: 88 | Qmax: 2.1466\n",
      "| Reward: -129 | Episode: 89 | Qmax: 2.2050\n",
      "| Reward: -131 | Episode: 90 | Qmax: 2.2297\n",
      "| Reward: -1 | Episode: 91 | Qmax: 2.1594\n",
      "| Reward: -124 | Episode: 92 | Qmax: 2.2532\n",
      "| Reward: -382 | Episode: 93 | Qmax: 2.2661\n",
      "| Reward: -3 | Episode: 94 | Qmax: 2.1381\n"
     ]
    }
   ],
   "source": [
    "%run ddpg.py --max-episodes 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_dim = 10\n",
    "a_dim = 4\n",
    "inputs = tflearn.input_data(shape=[None, s_dim])\n",
    "action = tflearn.input_data(shape=[None, a_dim])\n",
    "net = tflearn.fully_connected(inputs, 400)\n",
    "net = tflearn.layers.normalization.batch_normalization(net)\n",
    "net = tflearn.activations.relu(net)\n",
    "reluout = net\n",
    "# Add the action tensor in the 2nd hidden layer\n",
    "# Use two temp layers to get the corresponding weights and biases\n",
    "t1 = tflearn.fully_connected(net, 300)\n",
    "t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "net = tflearn.activation(\n",
    "    tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "actout = net\n",
    "# linear layer connected to 1 output representing Q(s,a)\n",
    "# Weights are init to Uniform[-3e-3, 3e-3]\n",
    "w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "out = tflearn.fully_connected(net, 1, weights_init=w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 Tensor(\"FullyConnected_19/BiasAdd:0\", shape=(?, 300), dtype=float32)\n",
      "t1.W <tf.Variable 'FullyConnected_19/W:0' shape=(400, 300) dtype=float32_ref>\n",
      "reluout Tensor(\"Relu_10:0\", shape=(?, 400), dtype=float32)\n",
      "actout Tensor(\"Relu_11:0\", shape=(?, 300), dtype=float32)\n",
      "matout Tensor(\"MatMul_13:0\", shape=(?, 300), dtype=float32)\n",
      "w_init <tensorflow.python.ops.init_ops.RandomUniform object at 0x7fba767abd30>\n",
      "out Tensor(\"FullyConnected_21/BiasAdd:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# print(inputs,action)\n",
    "print(\"t1\",t1)\n",
    "print(\"t1.W\",t1.W)\n",
    "print(\"reluout\",reluout)\n",
    "print(\"actout\",actout)\n",
    "matout = tf.matmul(reluout, t1.W)\n",
    "print(\"matout\",matout)\n",
    "print(\"w_init\",w_init)\n",
    "print(\"out\",out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant(0.)\n",
    "b = 2 * a\n",
    "g = tf.gradients(a + b, [a, b] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'gradients_3/AddN:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "lx = lambda x: tf.div(x, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'div_10:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lx(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--actor-lr ACTOR_LR]\n",
      "                             [--critic-lr CRITIC_LR] [--gamma GAMMA]\n",
      "                             [--tau TAU] [--buffer-size BUFFER_SIZE]\n",
      "                             [--minibatch-size MINIBATCH_SIZE] [--env ENV]\n",
      "                             [--random-seed RANDOM_SEED]\n",
      "                             [--max-episodes MAX_EPISODES]\n",
      "                             [--max-episode-len MAX_EPISODE_LEN]\n",
      "                             [--render-env] [--use-gym-monitor]\n",
      "                             [--monitor-dir MONITOR_DIR]\n",
      "                             [--summary-dir SUMMARY_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-9d770238-f696-4f62-89a3-62b17d6f5b3c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='provide arguments for DDPG agent')\n",
    "\n",
    "# agent parameters\n",
    "parser.add_argument('--actor-lr', help='actor network learning rate', default=0.0001)\n",
    "parser.add_argument('--critic-lr', help='critic network learning rate', default=0.001)\n",
    "parser.add_argument('--gamma', help='discount factor for critic updates', default=0.99)\n",
    "parser.add_argument('--tau', help='soft target update parameter', default=0.001)\n",
    "parser.add_argument('--buffer-size', help='max size of the replay buffer', default=1000000)\n",
    "parser.add_argument('--minibatch-size', help='size of minibatch for minibatch-SGD', default=64)\n",
    "\n",
    "# run parameters\n",
    "parser.add_argument('--env', help='choose the gym env- tested on {Pendulum-v0}', default='Pendulum-v0')\n",
    "parser.add_argument('--random-seed', help='random seed for repeatability', default=1234)\n",
    "parser.add_argument('--max-episodes', help='max num of episodes to do while training', default=50000)\n",
    "parser.add_argument('--max-episode-len', help='max length of 1 episode', default=1000)\n",
    "parser.add_argument('--render-env', help='render the gym env', action='store_true')\n",
    "parser.add_argument('--use-gym-monitor', help='record gym results', action='store_true')\n",
    "parser.add_argument('--monitor-dir', help='directory for storing gym results', default='./results/gym_ddpg')\n",
    "parser.add_argument('--summary-dir', help='directory for storing tensorboard info', default='./results/tf_ddpg')\n",
    "\n",
    "parser.set_defaults(render_env=False)\n",
    "parser.set_defaults(use_gym_monitor=True)\n",
    "\n",
    "args = vars(parser.parse_args())\n",
    "\n",
    "print (args.accumulate(args.integers))\n",
    "# pp.pprint(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sum] N [N ...]\n",
      "ipykernel_launcher.py: error: argument N: invalid int value: '/run/user/1000/jupyter/kernel-9d770238-f696-4f62-89a3-62b17d6f5b3c.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('integers', metavar='N', type=int, nargs='+',\n",
    "                    help='an integer for the accumulator')\n",
    "parser.add_argument('--sum', dest='accumulate', action='store_const',\n",
    "                    const=sum, default=max,\n",
    "                    help='sum the integers (default: find the max)')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print (args.echo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "state_dim 3 action_dim 1 action_bound [2.]\n",
      "action_bound type <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    np.random.seed(int('1234'))\n",
    "    tf.set_random_seed(int('1234'))\n",
    "    env.seed(int('1234'))\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = env.action_space.high\n",
    "    print(\"state_dim {} action_dim {} action_bound {}\".format(state_dim,action_dim,action_bound))\n",
    "    print(\"action_bound type {}\".format(type(action_bound)))\n",
    "    # Ensure action bound is symmetric\n",
    "    assert (env.action_space.high == -env.action_space.low)\n",
    "\n",
    "#     actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "#                          float(args['actor_lr']), float(args['tau']),\n",
    "#                          int(args['minibatch_size']))\n",
    "\n",
    "#     critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "#                            float(args['critic_lr']), float(args['tau']),\n",
    "#                            float(args['gamma']),\n",
    "#                            actor.get_num_trainable_vars())\n",
    "\n",
    "#     actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "#     if args['use_gym_monitor']:\n",
    "#         if not args['render_env']:\n",
    "#             env = wrappers.Monitor(\n",
    "#                 env, args['monitor_dir'], video_callable=False, force=True)\n",
    "#         else:\n",
    "#             env = wrappers.Monitor(env, args['monitor_dir'], force=True)\n",
    "\n",
    "#     train(sess, env, args, actor, critic, actor_noise)\n",
    "\n",
    "#     if args['use_gym_monitor']:\n",
    "#         env.monitor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "testaction=np.array([2.])\n",
    "testaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "# ===========================\n",
    "#   Actor and Critic DNNs\n",
    "# ===========================\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "            len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.fully_connected(net, 300)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(\n",
    "            net, self.a_dim, activation='tanh', weights_init=w_init)\n",
    "        # Scale output to -action_bound to action_bound\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "\n",
    "        # Add the action tensor in the 2nd hidden layer\n",
    "        # Use two temp layers to get the corresponding weights and biases\n",
    "        t1 = tflearn.fully_connected(net, 300)\n",
    "        t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "        net = tflearn.activation(\n",
    "            tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['env'] = \"Pendulum-v0\"\n",
    "args['random_seed'] = 1234\n",
    "args['actor_lr'] = 0.0001\n",
    "args['critic-lr'] = 0.001\n",
    "args['tau'] = 0.001\n",
    "args['minibatch_size'] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actor_lr': 0.0001,\n",
       " 'critic-lr': 0.001,\n",
       " 'env': 'Pendulum-v0',\n",
       " 'minibatch_size': 64,\n",
       " 'random_seed': 1234,\n",
       " 'tau': 0.001}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "state_dim 3 action_dim 1 action_bound [2.]\n",
      "actor.network_params length 10\n",
      "actor.target_network_params length 10\n",
      "actor.update_target_network_params length 10\n",
      "actor action_gradient Tensor(\"Placeholder:0\", shape=(?, 1), dtype=float32)\n",
      "actor unnormalized_actor_gradients [<tf.Tensor 'gradients/FullyConnected/MatMul_grad/MatMul_1:0' shape=(3, 400) dtype=float32>, <tf.Tensor 'gradients/FullyConnected/BiasAdd_grad/BiasAddGrad:0' shape=(400,) dtype=float32>, <tf.Tensor 'gradients/BatchNormalization/batchnorm/sub_grad/Reshape:0' shape=(400,) dtype=float32>, <tf.Tensor 'gradients/BatchNormalization/batchnorm/mul_grad/Reshape_1:0' shape=(400,) dtype=float32>, <tf.Tensor 'gradients/FullyConnected_1/MatMul_grad/MatMul_1:0' shape=(400, 300) dtype=float32>, <tf.Tensor 'gradients/FullyConnected_1/BiasAdd_grad/BiasAddGrad:0' shape=(300,) dtype=float32>, <tf.Tensor 'gradients/BatchNormalization_1/batchnorm/sub_grad/Reshape:0' shape=(300,) dtype=float32>, <tf.Tensor 'gradients/BatchNormalization_1/batchnorm/mul_grad/Reshape_1:0' shape=(300,) dtype=float32>, <tf.Tensor 'gradients/FullyConnected_2/MatMul_grad/MatMul_1:0' shape=(300, 1) dtype=float32>, <tf.Tensor 'gradients/FullyConnected_2/BiasAdd_grad/BiasAddGrad:0' shape=(1,) dtype=float32>]\n",
      "actor.scaled_out Tensor(\"Mul:0\", shape=(?, 1), dtype=float32)\n",
      "actor.action_gradient Tensor(\"Placeholder:0\", shape=(?, 1), dtype=float32)\n",
      "actor.network_params 10\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    env = gym.make(args['env'])\n",
    "    np.random.seed(int(args['random_seed']))\n",
    "    tf.set_random_seed(int(args['random_seed']))\n",
    "    env.seed(int(args['random_seed']))\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = env.action_space.high\n",
    "    # Ensure action bound is symmetric\n",
    "    assert (env.action_space.high == -env.action_space.low)\n",
    "    print(\"state_dim {} action_dim {} action_bound {}\".format(state_dim,action_dim,action_bound))\n",
    "    \n",
    "    actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                         float(args['actor_lr']), float(args['tau']),\n",
    "                         int(args['minibatch_size']))\n",
    "\n",
    "    print(\"actor.network_params length {}\".format(len(actor.network_params)))\n",
    "#     print(\"actor.network_params {}\".format(actor.network_params))\n",
    "    print(\"actor.target_network_params length {}\".format(len(actor.target_network_params)))\n",
    "    print(\"actor.update_target_network_params length {}\".format(len(actor.update_target_network_params)))\n",
    "    print(\"actor action_gradient {}\".format(actor.action_gradient))\n",
    "    print(\"actor unnormalized_actor_gradients {}\".format(actor.unnormalized_actor_gradients))\n",
    "    print(\"actor.scaled_out {}\".format(actor.scaled_out))\n",
    "    print(\"actor.action_gradient {}\".format(actor.action_gradient))\n",
    "    print(\"actor.network_params length {}\".format(len(actor.network_params)))\n",
    "    print(\"actor.network_params {}\".format(actor.network_params))\n",
    "    print(\"actor.actor_gradients {}\".format(actor.actor_gradients))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
